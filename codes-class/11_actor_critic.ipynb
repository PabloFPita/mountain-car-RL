{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "# Configuración warnings\n",
    "# ------------------------------------------------------------------------------\n",
    "import warnings\n",
    "#warnings.filterwarnings('once')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CartPole Environment\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the actor and critic networks\n",
    "actor = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(env.action_space.n, activation='softmax')\n",
    "])\n",
    "\n",
    "critic = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss functions\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "\n",
    "# Main training loop runs for a specified number of episodes (1000).\n",
    "# Agent interacts with the environment, and for each episode, it resets the environment and initializes the episode reward to 0.\n",
    "# The with tf.GradientTape block is used to compute gradients for the actor and critic networks.\n",
    "# Agent chooses an action based on the actor’s output probabilities and takes that action in the environment.\n",
    "# It observes the next state, reward, and whether the episode is done.\n",
    "# Advantage function is computed, which is the difference between the expected return and the estimated value at the current state.\n",
    "# Actor and Critic losses are calculated based on the advantage function.\n",
    "# Gradients are computed using tape.gradient and then applied to update the actor and critic networks using the respective optimisers.\n",
    "# Episode’s total reward is updated, and the loop continues until the episode ends.\n",
    "# Every 10 episodes, the current episode number and reward are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Episode 0, Reward: 11.0\n",
      "Episode 10, Reward: 16.0\n",
      "Episode 20, Reward: 8.0\n",
      "Episode 30, Reward: 13.0\n",
      "Episode 40, Reward: 15.0\n",
      "Episode 50, Reward: 51.0\n",
      "Episode 60, Reward: 23.0\n",
      "Episode 70, Reward: 12.0\n",
      "Episode 80, Reward: 25.0\n",
      "Episode 90, Reward: 46.0\n",
      "Episode 100, Reward: 18.0\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "num_episodes = 1000\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        for t in range(1, 10000): # Limit the number of time steps\n",
    "            # Choose an action using the actor\n",
    "            action_probs = actor(np.array([state]))\n",
    "            action = np.random.choice(env.action_space.n, p=action_probs.numpy()[0])\n",
    "\n",
    "            # Take the chosen action and observe the next state and reward\n",
    "            next_state, reward, done, _ , _= env.step(action)\n",
    "\n",
    "            # Compute the advantage\n",
    "            state_value = critic(np.array([state]))[0, 0]\n",
    "            next_state_value = critic(np.array([next_state]))[0, 0]\n",
    "            advantage = reward + gamma * next_state_value - state_value\n",
    "\n",
    "            # Compute actor and critic losses\n",
    "            actor_loss = -tf.math.log(action_probs[0, action]) * advantage\n",
    "            critic_loss = tf.square(advantage)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update actor and critic\n",
    "            actor_gradients = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "            critic_gradients = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "            actor_optimizer.apply_gradients(zip(actor_gradients, actor.trainable_variables))\n",
    "            critic_optimizer.apply_gradients(zip(critic_gradients, critic.trainable_variables))\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_jQ1tEQCxwRx"
   ],
   "name": "actor_critic.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
